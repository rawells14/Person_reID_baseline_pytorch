net output size:
torch.Size([8, 751])
[Resize(size=(256, 128), interpolation=PIL.Image.BICUBIC), Pad(padding=10, fill=0, padding_mode=constant), RandomCrop(size=(256, 128), padding=0), RandomHorizontalFlip(p=0.5), ToTensor(), Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]
/usr/local/lib/python3.6/dist-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.
  nn.init.kaiming_normal(m.weight.data)
0.32220983505249023
ft_net(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
  )
  (classifier): ClassBlock(
    (add_block): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=True)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): Dropout(p=0.5)
    )
    (classifier): Sequential(
      (0): Linear(in_features=512, out_features=751, bias=True)
    )
  )
)
Epoch 0/59
----------
train Loss: 5.2284 Acc: 0.1067
val Loss: 3.6080 Acc: 0.2237

Epoch 1/59
----------
train Loss: 2.8352 Acc: 0.3381
val Loss: 2.0948 Acc: 0.4434

Epoch 2/59
----------
train Loss: 1.9560 Acc: 0.5073
val Loss: 1.9898 Acc: 0.4687

Epoch 3/59
----------
train Loss: 1.5222 Acc: 0.5993
val Loss: 1.1405 Acc: 0.6618

Epoch 4/59
----------
train Loss: 1.2215 Acc: 0.6747
val Loss: 0.9145 Acc: 0.7057

Epoch 5/59
----------
train Loss: 1.0625 Acc: 0.7144
val Loss: 1.1573 Acc: 0.6671

Epoch 6/59
----------
train Loss: 0.8856 Acc: 0.7585
val Loss: 0.8792 Acc: 0.7377

Epoch 7/59
----------
train Loss: 0.8465 Acc: 0.7711
val Loss: 0.6172 Acc: 0.8083

Epoch 8/59
----------
train Loss: 0.7365 Acc: 0.7982
val Loss: 0.5457 Acc: 0.8349

Epoch 9/59
----------
train Loss: 0.6934 Acc: 0.8101
val Loss: 0.7802 Acc: 0.7563

Epoch 10/59
----------
train Loss: 0.6353 Acc: 0.8219
val Loss: 0.6162 Acc: 0.7909

Epoch 11/59
----------
train Loss: 0.6438 Acc: 0.8242
val Loss: 0.5080 Acc: 0.8309

Epoch 12/59
----------
train Loss: 0.5635 Acc: 0.8450
val Loss: 0.4233 Acc: 0.8628

Epoch 13/59
----------
train Loss: 0.5325 Acc: 0.8530
val Loss: 0.4509 Acc: 0.8469

Epoch 14/59
----------
train Loss: 0.5016 Acc: 0.8653
val Loss: 0.3454 Acc: 0.8788

Epoch 15/59
----------
train Loss: 0.4659 Acc: 0.8702
val Loss: 0.3357 Acc: 0.8935

Epoch 16/59
----------
train Loss: 0.4623 Acc: 0.8745
val Loss: 0.4175 Acc: 0.8655

Epoch 17/59
----------
train Loss: 0.4640 Acc: 0.8721
val Loss: 0.3057 Acc: 0.9015

Epoch 18/59
----------
train Loss: 0.4289 Acc: 0.8817
val Loss: 0.2881 Acc: 0.9028

Epoch 19/59
----------
train Loss: 0.3890 Acc: 0.8935
val Loss: 0.2592 Acc: 0.9121

Epoch 20/59
----------
train Loss: 0.3963 Acc: 0.8916
val Loss: 0.3196 Acc: 0.8855

Epoch 21/59
----------
train Loss: 0.3883 Acc: 0.8942
val Loss: 0.3194 Acc: 0.8935

Epoch 22/59
----------
train Loss: 0.3944 Acc: 0.8873
val Loss: 0.2225 Acc: 0.9174

Epoch 23/59
----------
train Loss: 0.3985 Acc: 0.8889
val Loss: 0.2734 Acc: 0.8988

Epoch 24/59
----------
train Loss: 0.3441 Acc: 0.9075
val Loss: 0.1797 Acc: 0.9281

Epoch 25/59
----------
train Loss: 0.2972 Acc: 0.9180
val Loss: 0.1669 Acc: 0.9374

Epoch 26/59
----------
train Loss: 0.2819 Acc: 0.9247
val Loss: 0.1726 Acc: 0.9348

Epoch 27/59
----------
train Loss: 0.3048 Acc: 0.9181
val Loss: 0.3628 Acc: 0.8828

Epoch 28/59
----------
train Loss: 0.4085 Acc: 0.8907
val Loss: 0.2493 Acc: 0.9095

Epoch 29/59
----------
train Loss: 0.3331 Acc: 0.9109
val Loss: 0.1653 Acc: 0.9387

Epoch 30/59
----------
train Loss: 0.2953 Acc: 0.9177
val Loss: 0.1676 Acc: 0.9348

Epoch 31/59
----------
train Loss: 0.2732 Acc: 0.9263
val Loss: 0.2528 Acc: 0.9134

Epoch 32/59
----------
train Loss: 0.3067 Acc: 0.9148
val Loss: 0.2698 Acc: 0.9055

Epoch 33/59
----------
train Loss: 0.3226 Acc: 0.9139
val Loss: 0.1973 Acc: 0.9228

Epoch 34/59
----------
train Loss: 0.3317 Acc: 0.9083
val Loss: 0.2517 Acc: 0.9188

Epoch 35/59
----------
train Loss: 0.2732 Acc: 0.9250
val Loss: 0.1954 Acc: 0.9294

Epoch 36/59
----------
train Loss: 0.2707 Acc: 0.9280
val Loss: 0.1749 Acc: 0.9268

Epoch 37/59
----------
train Loss: 0.2694 Acc: 0.9285
val Loss: 0.1401 Acc: 0.9414

Epoch 38/59
----------
train Loss: 0.2135 Acc: 0.9440
val Loss: 0.1169 Acc: 0.9454

Epoch 39/59
----------
train Loss: 0.2608 Acc: 0.9305
val Loss: 0.1556 Acc: 0.9308

Epoch 40/59
----------
train Loss: 0.0928 Acc: 0.9791
val Loss: 0.0125 Acc: 0.9787

Epoch 41/59
----------
train Loss: 0.0398 Acc: 0.9937
val Loss: 0.0058 Acc: 0.9800

Epoch 42/59
----------
train Loss: 0.0282 Acc: 0.9961
val Loss: 0.0049 Acc: 0.9800

Epoch 43/59
----------
train Loss: 0.0261 Acc: 0.9968
val Loss: 0.0048 Acc: 0.9800

Epoch 44/59
----------
train Loss: 0.0242 Acc: 0.9973
val Loss: 0.0045 Acc: 0.9800

Epoch 45/59
----------
train Loss: 0.0237 Acc: 0.9977
val Loss: 0.0043 Acc: 0.9800

Epoch 46/59
----------
train Loss: 0.0233 Acc: 0.9987
val Loss: 0.0042 Acc: 0.9800

Epoch 47/59
----------
train Loss: 0.0231 Acc: 0.9984
val Loss: 0.0044 Acc: 0.9800

Epoch 48/59
----------
train Loss: 0.0231 Acc: 0.9988
val Loss: 0.0047 Acc: 0.9800

Epoch 49/59
----------
train Loss: 0.0233 Acc: 0.9989
val Loss: 0.0052 Acc: 0.9800

Epoch 50/59
----------
train Loss: 0.0232 Acc: 0.9989
val Loss: 0.0053 Acc: 0.9800

Epoch 51/59
----------
train Loss: 0.0232 Acc: 0.9991
val Loss: 0.0058 Acc: 0.9800

Epoch 52/59
----------
train Loss: 0.0250 Acc: 0.9985
val Loss: 0.0062 Acc: 0.9800

Epoch 53/59
----------
train Loss: 0.0256 Acc: 0.9988
val Loss: 0.0061 Acc: 0.9800

Epoch 54/59
----------
train Loss: 0.0254 Acc: 0.9991
val Loss: 0.0064 Acc: 0.9800

Epoch 55/59
----------
train Loss: 0.0255 Acc: 0.9988
val Loss: 0.0069 Acc: 0.9800

Epoch 56/59
----------
train Loss: 0.0256 Acc: 0.9991
val Loss: 0.0072 Acc: 0.9800

Epoch 57/59
----------
train Loss: 0.0262 Acc: 0.9989
val Loss: 0.0074 Acc: 0.9800

Epoch 58/59
----------
train Loss: 0.0262 Acc: 0.9993
val Loss: 0.0072 Acc: 0.9800

Epoch 59/59
----------
train Loss: 0.0263 Acc: 0.9990
val Loss: 0.0080 Acc: 0.9800

Training complete in 76m 34s
